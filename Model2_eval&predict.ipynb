{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished imports!\n"
     ]
    }
   ],
   "source": [
    "#import things to use\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Masking, Dense \n",
    "from keras.layers import Convolution2D as Conv2D \n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D \n",
    "from keras.layers import Softmax, ReLU, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "from keras.optimizers import SGD\n",
    "print('finished imports!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#?\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tensorflow.ConfigProto()\n",
    "config.intra_op_parallelism_threads = 1\n",
    "print(config.intra_op_parallelism_threads)\n",
    "set_session(tensorflow.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made variable maxTime\n",
      "got data files in a list\n",
      "changed to files folder\n",
      "3559\n",
      "found trainingMax\n"
     ]
    }
   ],
   "source": [
    "def find_max(list_path, list_name, files_path): \n",
    "    #change to the directory where list of STFT files is \n",
    "    os.chdir(list_path)\n",
    "    #loop through to find maxTime, Don't need to save anything else though\n",
    "    maxTime = 0\n",
    "    print('made variable maxTime')\n",
    "\n",
    "    #open file (lists all train .npy STFT file names) \n",
    "    file = open(list_name, 'r') \n",
    "    data = file.read().split('\\n')\n",
    "    file.close()\n",
    "    print('got data files in a list')\n",
    "    #print(data)\n",
    "\n",
    "    os.chdir(files_path)\n",
    "    print('changed to files folder')\n",
    "\n",
    "    for array in data:\n",
    "        #find cols (number of time steps) of each STFT and save longest one\n",
    "        curSTFT = np.load(array)\n",
    "        rows, cols = curSTFT.shape\n",
    "        if (cols>maxTime):\n",
    "            maxTime = cols\n",
    "        if (rows!=1025):\n",
    "            print('Error, not 1025 STFT coefficients') \n",
    "    \n",
    "    return maxTime \n",
    "\n",
    "trainingMax = find_max('/home/ec2-user/SageMaker','trainListALL.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2')\n",
    "print(trainingMax)\n",
    "print('found trainingMax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with get_labels_get_samples\n"
     ]
    }
   ],
   "source": [
    "def get_labels_and_samples(path, file, datapath, maxTime):\n",
    "    #change to the directory where list of STFT files is \n",
    "    os.chdir(path)\n",
    "    \n",
    "    labels = []\n",
    "    paddedSTFTs = []\n",
    "    print('made variables')\n",
    "\n",
    "    #open file (lists all train .npy STFT file names) \n",
    "    file = open(file, 'r') #note, ran with 169 in TopList, could not handle all 676\n",
    "    data = file.read().split('\\n')\n",
    "    file.close()\n",
    "    print('got data files in a list')\n",
    "    print(data)\n",
    "\n",
    "    os.chdir(datapath)\n",
    "    print('changed to files folder')\n",
    "\n",
    "    for array in data:\n",
    "        #loop through to find out if file is a buzz or minibuzz, and add label accordingly \n",
    "        nameParse = array.split(\"_\",-1)\n",
    "        #print(nameParse)\n",
    "        typeParse = nameParse[4].split(\"u\", -1)\n",
    "        #print(typeParse)\n",
    "        if(typeParse[0] == 'b'):\n",
    "            #buzz = 1 \n",
    "            labels.append(1)\n",
    "        elif(typeParse[0] == 'minib'):\n",
    "            #minibuzz = 0\n",
    "            labels.append(0)\n",
    "        else: \n",
    "            print('Error, not a buzz or minibuzz!')\n",
    "\n",
    "    print(labels) \n",
    "    print('label loop done')\n",
    "\n",
    "    for array in data: \n",
    "        #loop though STFTs again to zero pad, transpose, and reshape soo that there is one channel  \n",
    "        #NOTE: must be done after we definitvely know the max number of time steps\n",
    "        curSTFT = np.load(array)\n",
    "        rows, cols = curSTFT.shape\n",
    "\n",
    "        zeroPad = np.zeros((rows,maxTime-cols))\n",
    "        paddedSTFT = np.append(curSTFT, zeroPad, axis = 1)\n",
    "\n",
    "        paddedSTFT = np.transpose(paddedSTFT)\n",
    "        paddedSTFT = np.reshape(paddedSTFT, paddedSTFT.shape + (1,))\n",
    "\n",
    "        paddedSTFTs.append(paddedSTFT)\n",
    "\n",
    "    print('sample loop done')\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    #one hot encoding for labels\n",
    "    hotlabels = keras.utils.to_categorical(labels, num_classes=2, dtype='float32')\n",
    "    samples = np.array(paddedSTFTs)\n",
    "    print('hotlabels and samples in np arrays')\n",
    "    return hotlabels, samples\n",
    "\n",
    "print('done with get_labels_get_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated training set\n",
      "generated vaildation set\n"
     ]
    }
   ],
   "source": [
    "def data_generator(path, file, datapath, bs, maxTime, mode='train'):\n",
    "    # open the text file for reading\n",
    "    os.chdir(path)\n",
    "    f = open(file, 'r')\n",
    "    while True: \n",
    "        \n",
    "        # initialize our batches of images and labels\n",
    "        samples = []\n",
    "        labels = []\n",
    "    \n",
    "        # keep looping until we reach our batch size\n",
    "        while len(samples) < bs:\n",
    "            # attempt to read the next line of the text file\n",
    "            line = f.readline()\n",
    "            \n",
    "            # check to see if the line is empty, indicating we have\n",
    "            # reached the end of the file\n",
    "            if line == \"\":\n",
    "                # reset the file pointer to the beginning of the file\n",
    "                # and re-read the line\n",
    "                f.seek(0)\n",
    "                line = f.readline()\n",
    "                # if we are evaluating we should now break from our\n",
    "\t\t\t\t# loop to ensure we don't continue to fill up the\n",
    "\t\t\t\t# batch from samples at the beginning of the file\n",
    "                if mode == \"eval\":\n",
    "                    break\n",
    "    \n",
    "            # construct list of labels: find out if file is a buzz or minibuzz, and add label accordingly \n",
    "            nameParse = line.split('_',-1)\n",
    "            #print(nameParse)\n",
    "            typeParse = nameParse[4].split('u', -1)\n",
    "            #print(typeParse)\n",
    "            if(typeParse[0] == 'b'):\n",
    "                #buzz = 1 \n",
    "                labels.append(1)\n",
    "            elif(typeParse[0] == 'minib'):\n",
    "                #minibuzz = 0\n",
    "                labels.append(0)\n",
    "            else: \n",
    "                print('Error, not a buzz or minibuzz!')\n",
    "    \n",
    "            #switch to aws_pilotwhales2 folder\n",
    "            os.chdir(datapath)\n",
    "            #print('changed to aws_pilotwhales2 folder')\n",
    "            \n",
    "            #construct list of samples\n",
    "            lineParse = line.split(\"\\n\",-1)\n",
    "            #print(lineParse)\n",
    "            #print(os.getcwd())\n",
    "            curSTFT = np.load(lineParse[0])\n",
    "            rows, cols = curSTFT.shape\n",
    "    \n",
    "            zeroPad = np.zeros((rows,maxTime-cols))\n",
    "            paddedSTFT = np.append(curSTFT, zeroPad, axis = 1)\n",
    "    \n",
    "            paddedSTFT = np.transpose(paddedSTFT)\n",
    "            paddedSTFT = np.reshape(paddedSTFT, paddedSTFT.shape + (1,))\n",
    "    \n",
    "            samples.append(paddedSTFT)\n",
    "        \n",
    "        #convert from lists to numpy arrays\n",
    "        labels = np.array(labels)\n",
    "        #one hot encoding for labels\n",
    "        hotlabels = keras.utils.to_categorical(labels, num_classes=2, dtype='float32')\n",
    "        samples = np.array(samples)\n",
    "        #print('hotlabels and samples in np arrays')\n",
    "    \n",
    "        # yield the batch to the calling function\n",
    "        if mode == \"predict\":\n",
    "            yield(samples)\n",
    "        else: \n",
    "            yield (samples, hotlabels)\n",
    "        \n",
    "# initialize both the training and validation generators\n",
    "trainGen = data_generator('/home/ec2-user/SageMaker', 'trainList.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2', 13, trainingMax, mode = 'train')\n",
    "print('generated training set')\n",
    "validGen = data_generator('/home/ec2-user/SageMaker', 'validList.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2', 13, trainingMax, mode = 'train')\n",
    "print('generated vaildation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to start model\n",
      "made model\n",
      "3559\n",
      "layer 1 done\n",
      "layer 2 done\n",
      "layer 3 done\n",
      "layer 4 done\n",
      "layer 5 done\n",
      "layer 6 done\n",
      "compiled\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 3559, 1, 256)      262400    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 3559, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1780, 1, 256)      196608    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 1780, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 890, 1, 256)       196608    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 890, 1, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 445, 1, 256)       196608    \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 445, 1, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 512       \n",
      "=================================================================\n",
      "Total params: 852,736\n",
      "Trainable params: 852,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#MAKE MODEL AND COMPILE IT\n",
    "\n",
    "#layer 0: input\n",
    "#labels[], samples[] (maxTime rows, 1025 cols) each in trainGen and validGen\n",
    "print('ready to start model')\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "print('made model')\n",
    "\n",
    "#NOTE, CHANGED PADDING ON 2D CONVOLUTIONS from 'valid'=no paddinng to 'same'=padding so input and output are same dimensions\n",
    "\n",
    "#layer 1: 2D convolution between input and 256 filters with 1 row and 1025 cols\n",
    "print(trainingMax)\n",
    "model.add(Conv2D(256, input_shape = [trainingMax,1025,1], kernel_size = [1,1025], strides=(1, 1), padding='valid', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "#batch normalization- add in layer? don't understand parameters well\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "#reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 1 done')\n",
    "\n",
    "##layer 2: 2D convolution between output of layer 1 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 2 done')\n",
    "\n",
    "#layer 3: 2D convolution between output of layer 2 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 3 done')\n",
    "\n",
    "#layer 4: 2D convolution between output of layer 3 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 4 done')\n",
    "\n",
    "#layer 5: Global max pooling\n",
    "model.add(GlobalMaxPooling2D(data_format=\"channels_last\"))\n",
    "print('layer 5 done')\n",
    "\n",
    "#layer 6: fully connected layer\n",
    "model.add(Dense(2, activation='softmax', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "print('layer 6 done')\n",
    "\n",
    "#Compile model [COMPILE]\n",
    "#OLD COMPILE (for fit, not fit_generator)\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "opt = SGD(lr=0.02) #note, can play with leraning rate and other parameters here\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics=[\"accuracy\"])\n",
    "print('compiled')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training w/ generator...\n",
      "Epoch 1/5\n",
      " - 104s - loss: 0.6582 - acc: 0.8462 - val_loss: 0.5955 - val_acc: 0.9308\n",
      "Epoch 2/5\n",
      " - 94s - loss: 0.4960 - acc: 0.9084 - val_loss: 0.3554 - val_acc: 0.9462\n",
      "Epoch 3/5\n",
      " - 95s - loss: 0.3415 - acc: 0.9359 - val_loss: 0.2099 - val_acc: 0.9769\n",
      "Epoch 4/5\n",
      " - 94s - loss: 0.2825 - acc: 0.9359 - val_loss: 0.1544 - val_acc: 0.9769\n",
      "Epoch 5/5\n",
      " - 94s - loss: 0.2440 - acc: 0.9505 - val_loss: 0.1167 - val_acc: 0.9769\n",
      "ran fit_generator\n"
     ]
    }
   ],
   "source": [
    "#Now let us train our model [FIT]\n",
    "ES = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "#with fit_generator\n",
    "print(\"[INFO] training w/ generator...\")\n",
    "model.fit_generator(trainGen, steps_per_epoch=42, epochs=5, verbose=2, callbacks=[ES], validation_data=validGen, validation_steps=10, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "print('ran fit_generator')\n",
    "\n",
    "#with fit\n",
    "#model.fit(x=samples, y=hotlabels, batch_size=13, epochs=5, verbose=2, callbacks=[ES], validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#print('ran fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated testing set\n"
     ]
    }
   ],
   "source": [
    "#Prepare test data for evaluation\n",
    "\n",
    "#NOTE NEED TO USE TRAINING MAX TO BE CONSISTENT WITH THE NEURAL NETWORK\n",
    "#find max length (time) in testing data\n",
    "#testingMax = find_max('/home/ec2-user/SageMaker','testList.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2_testData')\n",
    "#maxTime = testingMax\n",
    "#print(testingMax)\n",
    "#print('found testingMax')\n",
    "\n",
    "# initialize testing generator\n",
    "testGen = data_generator('/home/ec2-user/SageMaker', 'testList.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2_testData', 13, trainingMax, mode = \"eval\")\n",
    "print('generated testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 14s 1s/step\n",
      "[0.14795524455033815, 0.9763313623575064]\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "#Now let us evaluate our model [EVALUATE]\n",
    "\n",
    "#with evaluate_generator\n",
    "results = model.evaluate_generator(testGen, steps = 13, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "print(results)\n",
    "print(model.metrics_names)\n",
    "\n",
    "#with evauluate\n",
    "#results = model.evaluate(x=testSamples, y=testHotlabels, batch_size=26, verbose=1, sample_weight=None, steps=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made variables\n",
      "got data files in a list\n",
      "['STFT_F_gm266a_event1016_buzz344.npy', 'STFT_F_gm185b_event415_minibuzz148.npy', 'STFT_F_gm267a_event706_buzz176.npy', 'STFT_F_gm208a_event236_minibuzz4.npy', 'STFT_F_gm266a_event939_buzz280.npy', 'STFT_F_gm185b_event423_minibuzz154.npy', 'STFT_F_gm267a_event336_buzz20.npy', 'STFT_F_gm187b_event2312_minibuzz147.npy', 'STFT_F_gm266a_event1078_buzz394.npy', 'STFT_F_gm187b_event336_minibuzz33.npy', 'STFT_F_gm266a_event1298_buzz555.npy', 'STFT_F_gm185b_event299_minibuzz94.npy', 'STFT_F_gm267a_event1635_buzz436.npy', 'STFT_F_gm185b_event389_minibuzz138.npy', 'STFT_F_gm266a_event1119_buzz422.npy', 'STFT_F_gm187b_event1919_minibuzz124.npy', 'STFT_F_gm267a_event460_buzz68.npy', 'STFT_F_gm185b_event256_minibuzz77.npy', 'STFT_F_gm208a_event660_buzz85.npy', 'STFT_F_gm208a_event100_minibuzz3.npy', 'STFT_F_gm187b_event660_buzz84.npy', 'STFT_F_gm185b_event398_minibuzz142.npy', 'STFT_F_gm267a_event846_buzz251.npy', 'STFT_F_gm185b_event262_minibuzz81.npy', 'STFT_F_gm266a_event796_buzz183.npy', 'STFT_F_gm185b_event205_minibuzz47.npy', 'STFT_F_gm266a_event1129_buzz432.npy', 'STFT_F_gm187b_event48_minibuzz3.npy', 'STFT_F_gm266a_event521_buzz87.npy', 'STFT_F_gm187b_event2314_minibuzz148.npy', 'STFT_F_gm267a_event550_buzz85.npy', 'STFT_F_gm187b_event1589_minibuzz83.npy', 'STFT_F_gm266a_event516_buzz82.npy', 'STFT_F_gm185b_event232_minibuzz64.npy', 'STFT_F_gm267a_event623_buzz130.npy', 'STFT_F_gm185b_event175_minibuzz26.npy', 'STFT_F_gm267a_event1513_buzz379.npy', 'STFT_F_gm187b_event1925_minibuzz125.npy', 'STFT_F_gm266a_event511_buzz77.npy', 'STFT_F_gm188a_event30_minibuzz4.npy', 'STFT_F_gm266a_event717_buzz109.npy', 'STFT_F_gm185b_event353_minibuzz110.npy', 'STFT_F_gm185b_event460_buzz13.npy', 'STFT_F_gm187b_event2186_minibuzz136.npy', 'STFT_F_gm266a_event141_buzz23.npy', 'STFT_F_gm187a_event260_minibuzz5.npy', 'STFT_F_gm266a_event1290_buzz547.npy', 'STFT_F_gm185b_event432_minibuzz160.npy', 'STFT_F_gm209c_event92_buzz3.npy', 'STFT_F_gm185b_event171_minibuzz23.npy', 'STFT_F_gm187b_event192_buzz38.npy', 'STFT_F_gm208a_event406_minibuzz12.npy', 'STFT_F_gm266a_event477_buzz70.npy', 'STFT_F_gm185b_event228_minibuzz61.npy', 'STFT_F_gm266a_event1045_buzz361.npy', 'STFT_F_gm187b_event197_minibuzz22.npy', 'STFT_F_gm267a_event1623_buzz424.npy', 'STFT_F_gm187a_event173_minibuzz4.npy', 'STFT_F_gm187b_event1408_buzz177.npy', 'STFT_F_gm187b_event1416_minibuzz77.npy', 'STFT_F_gm266a_event1258_buzz533.npy', 'STFT_F_gm186b_event39_minibuzz3.npy', 'STFT_F_gm208a_event369_buzz19.npy', 'STFT_F_gm187b_event765_minibuzz61.npy', 'STFT_F_gm266a_event772_buzz161.npy', 'STFT_F_gm186b_event41_minibuzz5.npy', 'STFT_F_gm266a_event227_buzz38.npy', 'STFT_F_gm185b_event184_minibuzz33.npy', 'STFT_F_gm267a_event350_buzz33.npy', 'STFT_F_gm187b_event2101_minibuzz128.npy', 'STFT_F_gm209c_event577_buzz14.npy', 'STFT_F_gm187b_event202_minibuzz25.npy', 'STFT_F_gm209a_event107_buzz36.npy', 'STFT_F_gm187b_event2141_minibuzz133.npy', 'STFT_F_gm185b_event537_buzz30.npy', 'STFT_F_gm185b_event163_minibuzz16.npy', 'STFT_F_gm266a_event726_buzz118.npy', 'STFT_F_gm187b_event2100_minibuzz127.npy', 'STFT_F_gm267a_event342_buzz25.npy', 'STFT_F_gm187b_event1595_minibuzz87.npy', 'STFT_F_gm267a_event215_buzz7.npy', 'STFT_F_gm208a_event767_minibuzz22.npy', 'STFT_F_gm266a_event451_buzz65.npy', 'STFT_F_gm185b_event203_minibuzz46.npy', 'STFT_F_gm266a_event1323_buzz563.npy', 'STFT_F_gm187b_event728_minibuzz54.npy', 'STFT_F_gm208a_event578_buzz60.npy', 'STFT_F_gm185b_event367_minibuzz121.npy', 'STFT_F_gm266a_event892_buzz258.npy', 'STFT_F_gm187b_event744_minibuzz57.npy', 'STFT_F_gm209c_event578_buzz15.npy', 'STFT_F_gm187b_event176_minibuzz20.npy', 'STFT_F_gm267a_event1559_buzz409.npy', 'STFT_F_gm187b_event2536_minibuzz172.npy', 'STFT_F_gm187a_event159_buzz3.npy', 'STFT_F_gm185b_event200_minibuzz44.npy', 'STFT_F_gm266a_event1326_buzz566.npy', 'STFT_F_gm185b_event349_minibuzz108.npy', 'STFT_F_gm208a_event740_buzz110.npy', 'STFT_F_gm185b_event483_minibuzz164.npy', 'STFT_F_gm267a_event1506_buzz372.npy', 'STFT_F_gm187b_event534_minibuzz44.npy', 'STFT_F_gm266a_event998_buzz326.npy', 'STFT_F_gm185b_event181_minibuzz30.npy', 'STFT_F_gm266a_event727_buzz119.npy', 'STFT_F_gm187b_event306_minibuzz27.npy', 'STFT_F_gm208a_event496_buzz45.npy', 'STFT_F_gm187b_event1600_minibuzz89.npy', 'STFT_F_gm266a_event1359_buzz575.npy', 'STFT_F_gm208a_event1149_minibuzz29.npy', 'STFT_F_gm267a_event1442_buzz360.npy', 'STFT_F_gm185b_event147_minibuzz2.npy', 'STFT_F_gm187b_event1479_buzz185.npy', 'STFT_F_gm187b_event1848_minibuzz116.npy', 'STFT_F_gm266a_event881_buzz247.npy', 'STFT_F_gm185b_event394_minibuzz140.npy', 'STFT_F_gm266a_event1247_buzz522.npy', 'STFT_F_gm187b_event198_minibuzz23.npy', 'STFT_F_gm267a_event789_buzz236.npy', 'STFT_F_gm185b_event281_minibuzz90.npy', 'STFT_F_gm267a_event1303_buzz342.npy', 'STFT_F_gm187b_event872_minibuzz68.npy', 'STFT_F_gm266a_event120_buzz2.npy', 'STFT_F_gm187b_event2429_minibuzz166.npy', 'STFT_F_gm266a_event1248_buzz523.npy', 'STFT_F_gm185b_event429_minibuzz157.npy', 'STFT_F_gm267a_event1549_buzz399.npy', 'STFT_F_gm187b_event337_minibuzz34.npy', 'STFT_F_gm266a_event523_buzz89.npy', 'STFT_F_gm187b_event54_minibuzz9.npy', 'STFT_F_gm267a_event474_buzz82.npy', 'STFT_F_gm185b_event219_minibuzz55.npy', 'STFT_F_gm187b_event2341_buzz241.npy', 'STFT_F_gm187b_event2555_minibuzz175.npy', 'STFT_F_gm266a_event1025_buzz348.npy', 'STFT_F_gm185b_event226_minibuzz60.npy', 'STFT_F_gm267a_event668_buzz155.npy', 'STFT_F_gm185b_event424_minibuzz155.npy', 'STFT_F_gm266a_event1200_buzz491.npy', 'STFT_F_gm185b_event378_minibuzz131.npy', 'STFT_F_gm187b_event2348_buzz246.npy', 'STFT_F_gm187b_event2192_minibuzz140.npy', 'STFT_F_gm187b_event201_buzz43.npy', 'STFT_F_gm185b_event315_minibuzz103.npy', 'STFT_F_gm266a_event740_buzz129.npy', 'STFT_F_gm185b_event193_minibuzz38.npy', 'STFT_F_gm187b_event1150_buzz131.npy', 'STFT_F_gm187b_event358_minibuzz37.npy', 'STFT_F_gm267a_event1516_buzz382.npy', 'STFT_F_gm187b_event175_minibuzz19.npy', 'STFT_F_gm266a_event767_buzz156.npy', 'STFT_F_gm187b_event2644_minibuzz182.npy', 'STFT_F_gm266a_event832_buzz219.npy', 'STFT_F_gm185b_event297_minibuzz92.npy', 'STFT_F_gm266a_event182_buzz33.npy', 'STFT_F_gm187b_event2189_minibuzz138.npy', 'STFT_F_gm267a_event768_buzz215.npy', 'STFT_F_gm185b_event266_minibuzz83.npy', 'STFT_F_gm208a_event438_buzz32.npy', 'STFT_F_gm187b_event764_minibuzz60.npy', 'STFT_F_gm266a_event530_buzz96.npy', 'STFT_F_gm185b_event273_minibuzz85.npy', 'STFT_F_gm267a_event975_buzz263.npy', 'STFT_F_gm187b_event2197_minibuzz141.npy', 'STFT_F_gm266a_event1118_buzz421.npy', 'STFT_F_gm187b_event2646_minibuzz183.npy', 'STFT_F_gm267a_event1301_buzz340.npy', 'STFT_F_gm187a_event166_minibuzz2.npy', 'STFT_F_gm187b_event112_buzz26.npy', 'STFT_F_gm187b_event2055_minibuzz126.npy']\n",
      "changed to files folder\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "label loop done\n",
      "sample loop done\n",
      "hotlabels and samples in np arrays\n",
      "170/170 [==============================] - 12s 70ms/step\n"
     ]
    }
   ],
   "source": [
    "#[PREDICT] (w/TestData I kept aside as well)\n",
    "\n",
    "#predictGen = data_generator('/home/ec2-user/SageMaker', 'testList.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2_testData', 13, trainingMax, mode = \"eval\")\n",
    "#print('generated predict set')\n",
    "\n",
    "#with predict_generator\n",
    "#need a different generator(just grab batches of data but do not know answer)\n",
    "#predictions = model.predict_generator(testGen, steps=13, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "#print(predictions)\n",
    "\n",
    "#with predict\n",
    "#preprocess STFT data in TestData folder!\n",
    "#see getLabels_stackedData3.py, but don't give labels\n",
    "hotlabels, samples = get_labels_and_samples('/home/ec2-user/SageMaker', 'testList.txt', '/home/ec2-user/SageMaker/aws_pilotwhales2_testData', trainingMax)\n",
    "hotlabels_pred = model.predict(samples, batch_size=13, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[2.40261108e-02 9.75973904e-01]\n",
      " [9.85730529e-01 1.42694665e-02]\n",
      " [1.04926759e-02 9.89507377e-01]\n",
      " [6.12970352e-01 3.87029648e-01]\n",
      " [9.83259524e-05 9.99901652e-01]\n",
      " [8.19318175e-01 1.80681780e-01]\n",
      " [3.40281069e-01 6.59718931e-01]\n",
      " [7.04672337e-01 2.95327723e-01]\n",
      " [3.06612370e-03 9.96933818e-01]\n",
      " [9.97876883e-01 2.12311768e-03]\n",
      " [3.30185965e-02 9.66981411e-01]\n",
      " [9.27680671e-01 7.23193213e-02]\n",
      " [7.08818762e-03 9.92911756e-01]\n",
      " [9.99740541e-01 2.59491790e-04]\n",
      " [2.11093798e-01 7.88906157e-01]\n",
      " [9.99687433e-01 3.12529068e-04]\n",
      " [3.77866998e-03 9.96221304e-01]\n",
      " [9.96991873e-01 3.00813606e-03]\n",
      " [1.93653345e-01 8.06346655e-01]\n",
      " [7.86902845e-01 2.13097081e-01]\n",
      " [2.47077569e-01 7.52922475e-01]\n",
      " [9.96346176e-01 3.65381432e-03]\n",
      " [3.01980749e-02 9.69801903e-01]\n",
      " [9.21250105e-01 7.87498653e-02]\n",
      " [3.11312415e-02 9.68868732e-01]\n",
      " [7.74845302e-01 2.25154683e-01]\n",
      " [3.81132821e-03 9.96188700e-01]\n",
      " [9.94258881e-01 5.74114546e-03]\n",
      " [1.07408669e-02 9.89259183e-01]\n",
      " [7.72338808e-01 2.27661163e-01]\n",
      " [7.68809160e-03 9.92311895e-01]\n",
      " [9.90190864e-01 9.80912708e-03]\n",
      " [2.43038788e-01 7.56961226e-01]\n",
      " [9.63779986e-01 3.62200066e-02]\n",
      " [2.29086704e-03 9.97709155e-01]\n",
      " [5.48050940e-01 4.51949030e-01]\n",
      " [1.90895990e-01 8.09104085e-01]\n",
      " [9.99973774e-01 2.62276335e-05]\n",
      " [1.65986340e-03 9.98340130e-01]\n",
      " [9.05934572e-01 9.40654278e-02]\n",
      " [1.85655486e-02 9.81434464e-01]\n",
      " [8.58635724e-01 1.41364276e-01]\n",
      " [1.89196914e-02 9.81080234e-01]\n",
      " [9.98422980e-01 1.57698512e-03]\n",
      " [2.96001174e-02 9.70399857e-01]\n",
      " [9.81703699e-01 1.82962734e-02]\n",
      " [4.71728941e-04 9.99528289e-01]\n",
      " [8.49572957e-01 1.50427014e-01]\n",
      " [1.45794556e-01 8.54205430e-01]\n",
      " [6.11963034e-01 3.88037026e-01]\n",
      " [2.92552531e-01 7.07447469e-01]\n",
      " [9.44470048e-01 5.55299930e-02]\n",
      " [5.75780810e-04 9.99424219e-01]\n",
      " [9.87983823e-01 1.20161446e-02]\n",
      " [1.21138855e-05 9.99987841e-01]\n",
      " [1.07329890e-01 8.92670155e-01]\n",
      " [1.23468302e-02 9.87653136e-01]\n",
      " [9.52240586e-01 4.77594063e-02]\n",
      " [8.51840734e-01 1.48159310e-01]\n",
      " [9.52648461e-01 4.73515689e-02]\n",
      " [3.29341516e-02 9.67065871e-01]\n",
      " [9.78673756e-01 2.13262904e-02]\n",
      " [1.83083594e-05 9.99981642e-01]\n",
      " [9.99963284e-01 3.66819841e-05]\n",
      " [1.44869061e-02 9.85513151e-01]\n",
      " [9.79193389e-01 2.08065677e-02]\n",
      " [2.16025865e-06 9.99997854e-01]\n",
      " [9.28357422e-01 7.16425627e-02]\n",
      " [1.29157417e-02 9.87084210e-01]\n",
      " [9.99854207e-01 1.45729660e-04]\n",
      " [3.15981060e-02 9.68401909e-01]\n",
      " [9.93528306e-01 6.47165207e-03]\n",
      " [7.44508754e-04 9.99255478e-01]\n",
      " [9.99811709e-01 1.88278820e-04]\n",
      " [5.15174428e-08 1.00000000e+00]\n",
      " [5.79902530e-01 4.20097500e-01]\n",
      " [1.19034406e-02 9.88096595e-01]\n",
      " [9.99548376e-01 4.51589265e-04]\n",
      " [2.34761480e-02 9.76523817e-01]\n",
      " [8.83285999e-01 1.16714016e-01]\n",
      " [4.54405367e-01 5.45594633e-01]\n",
      " [9.11153376e-01 8.88466537e-02]\n",
      " [1.33630650e-07 9.99999881e-01]\n",
      " [6.04577303e-01 3.95422727e-01]\n",
      " [6.85150444e-04 9.99314904e-01]\n",
      " [8.52661192e-01 1.47338808e-01]\n",
      " [2.91420259e-02 9.70857978e-01]\n",
      " [9.95682478e-01 4.31757374e-03]\n",
      " [6.65024738e-04 9.99334991e-01]\n",
      " [8.80163968e-01 1.19835980e-01]\n",
      " [2.68638287e-05 9.99973178e-01]\n",
      " [9.99951243e-01 4.87525540e-05]\n",
      " [9.10327770e-03 9.90896761e-01]\n",
      " [9.01527882e-01 9.84721407e-02]\n",
      " [9.88036513e-01 1.19634196e-02]\n",
      " [7.36966550e-01 2.63033450e-01]\n",
      " [2.79004264e-10 1.00000000e+00]\n",
      " [9.98985350e-01 1.01468549e-03]\n",
      " [5.85762262e-01 4.14237648e-01]\n",
      " [7.12388217e-01 2.87611812e-01]\n",
      " [4.47874330e-03 9.95521307e-01]\n",
      " [7.38313854e-01 2.61686236e-01]\n",
      " [1.26392406e-03 9.98736084e-01]\n",
      " [7.33215332e-01 2.66784668e-01]\n",
      " [1.15848510e-02 9.88415122e-01]\n",
      " [7.75879562e-01 2.24120408e-01]\n",
      " [1.33182362e-01 8.66817653e-01]\n",
      " [8.94787252e-01 1.05212808e-01]\n",
      " [1.21897215e-03 9.98781025e-01]\n",
      " [9.88267839e-01 1.17321257e-02]\n",
      " [1.57876648e-02 9.84212339e-01]\n",
      " [7.90960252e-01 2.09039807e-01]\n",
      " [1.43220931e-01 8.56779039e-01]\n",
      " [8.49701047e-01 1.50299013e-01]\n",
      " [2.21068963e-01 7.78931022e-01]\n",
      " [9.99860406e-01 1.39532247e-04]\n",
      " [1.10146310e-02 9.88985419e-01]\n",
      " [8.58715057e-01 1.41284913e-01]\n",
      " [4.10936121e-03 9.95890617e-01]\n",
      " [7.96293199e-01 2.03706786e-01]\n",
      " [4.68575256e-03 9.95314240e-01]\n",
      " [8.43008041e-01 1.56991929e-01]\n",
      " [8.64560279e-05 9.99913573e-01]\n",
      " [8.52147162e-01 1.47852883e-01]\n",
      " [4.12997603e-02 9.58700240e-01]\n",
      " [9.98261154e-01 1.73889671e-03]\n",
      " [4.20479961e-02 9.57951963e-01]\n",
      " [9.98794436e-01 1.20552815e-03]\n",
      " [5.90285882e-02 9.40971434e-01]\n",
      " [1.00000000e+00 2.57623949e-08]\n",
      " [1.05414993e-05 9.99989510e-01]\n",
      " [8.36389244e-01 1.63610756e-01]\n",
      " [4.15500790e-01 5.84499180e-01]\n",
      " [8.97045910e-01 1.02954149e-01]\n",
      " [6.10172574e-04 9.99389768e-01]\n",
      " [7.92500198e-01 2.07499877e-01]\n",
      " [3.13340127e-03 9.96866643e-01]\n",
      " [9.54839766e-01 4.51602489e-02]\n",
      " [4.85407450e-04 9.99514580e-01]\n",
      " [9.96355534e-01 3.64450109e-03]\n",
      " [1.62215456e-01 8.37784469e-01]\n",
      " [7.95575321e-01 2.04424664e-01]\n",
      " [1.11403624e-02 9.88859653e-01]\n",
      " [8.65327597e-01 1.34672359e-01]\n",
      " [1.03446509e-05 9.99989629e-01]\n",
      " [5.79867840e-01 4.20132160e-01]\n",
      " [1.60029985e-03 9.98399675e-01]\n",
      " [9.99999881e-01 1.13721697e-07]\n",
      " [3.12210441e-01 6.87789619e-01]\n",
      " [9.98945177e-01 1.05483364e-03]\n",
      " [2.43757255e-02 9.75624323e-01]\n",
      " [9.52482760e-01 4.75172400e-02]\n",
      " [6.30211376e-04 9.99369800e-01]\n",
      " [9.29385841e-01 7.06141517e-02]\n",
      " [1.24694488e-03 9.98753071e-01]\n",
      " [9.00344789e-01 9.96551588e-02]\n",
      " [1.68310441e-02 9.83168960e-01]\n",
      " [9.70587373e-01 2.94126607e-02]\n",
      " [6.23341128e-02 9.37665820e-01]\n",
      " [9.99595940e-01 4.04100341e-04]\n",
      " [5.14960084e-05 9.99948502e-01]\n",
      " [9.77607906e-01 2.23920532e-02]\n",
      " [3.52914259e-02 9.64708567e-01]\n",
      " [9.41245556e-01 5.87544553e-02]\n",
      " [1.79495916e-01 8.20504069e-01]\n",
      " [9.13105786e-01 8.68941620e-02]\n",
      " [1.61105376e-02 9.83889461e-01]\n",
      " [1.00000000e+00 3.43997364e-08]\n",
      " [1.20113484e-07 9.99999881e-01]\n",
      " [9.67070997e-01 3.29289734e-02]]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "#see if prediction results are right (compare hot labels I generate with what the model guesses)\n",
    "print(hotlabels)\n",
    "hotlabels = keras.utils.to_categorical(hotlabels, num_classes=2, dtype='float32')\n",
    "print(hotlabels)\n",
    "print(hotlabels_pred)\n",
    "print((np.max(abs(hotlabels-hotlabels_pred),axis=1)>.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
