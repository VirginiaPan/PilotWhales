{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished imports!\n"
     ]
    }
   ],
   "source": [
    "#import things to use\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Masking, Dense \n",
    "from keras.layers import Convolution2D as Conv2D \n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D \n",
    "from keras.layers import Softmax, ReLU, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "print('finished imports!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#?\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tensorflow.ConfigProto()\n",
    "config.intra_op_parallelism_threads = 1\n",
    "print(config.intra_op_parallelism_threads)\n",
    "set_session(tensorflow.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n",
      "/home/ec2-user/SageMaker\n",
      "made variables\n",
      "got data files in a list\n",
      "['STFT_F_gm266a_event732_buzz124.npy', 'STFT_F_gm185b_event178_minibuzz28.npy', 'STFT_F_gm266a_event1202_buzz493.npy', 'STFT_F_gm185b_event199_minibuzz43.npy', 'STFT_F_gm267a_event1031_buzz276.npy', 'STFT_F_gm185b_event249_minibuzz74.npy', 'STFT_F_gm266a_event1204_buzz495.npy', 'STFT_F_gm208a_event443_minibuzz14.npy', 'STFT_F_gm267a_event1521_buzz387.npy', 'STFT_F_gm187b_event1602_minibuzz91.npy', 'STFT_F_gm266a_event1136_buzz439.npy', 'STFT_F_gm187b_event815_minibuzz62.npy', 'STFT_F_gm187b_event2612_buzz259.npy', 'STFT_F_gm185b_event222_minibuzz57.npy', 'STFT_F_gm267a_event343_buzz26.npy', 'STFT_F_gm208a_event240_minibuzz7.npy', 'STFT_F_gm208a_event666_buzz88.npy', 'STFT_F_gm187b_event1613_minibuzz96.npy', 'STFT_F_gm187b_event190_buzz36.npy', 'STFT_F_gm187b_event2588_minibuzz177.npy', 'STFT_F_gm266a_event1051_buzz367.npy', 'STFT_F_gm185b_event241_minibuzz69.npy', 'STFT_F_gm185b_event466_buzz16.npy', 'STFT_F_gm208a_event446_minibuzz17.npy', 'STFT_F_gm187b_event1294_buzz156.npy', 'STFT_F_gm187b_event2423_minibuzz162.npy', 'STFT_F_gm266a_event1183_buzz478.npy', 'STFT_F_gm187b_event1694_minibuzz108.npy', 'STFT_F_gm267a_event557_buzz92.npy', 'STFT_F_gm188a_event9_minibuzz1.npy', 'STFT_F_gm267a_event1515_buzz381.npy', 'STFT_F_gm208a_event444_minibuzz15.npy', 'STFT_F_gm187b_event2613_buzz260.npy', 'STFT_F_gm187b_event2139_minibuzz131.npy', 'STFT_F_gm267a_event773_buzz220.npy', 'STFT_F_gm208a_event98_minibuzz2.npy', 'STFT_F_gm267a_event395_buzz49.npy', 'STFT_F_gm208a_event1146_minibuzz28.npy', 'STFT_F_gm267a_event1698_buzz457.npy', 'STFT_F_gm187b_event50_minibuzz5.npy', 'STFT_F_gm187b_event1153_buzz134.npy', 'STFT_F_gm187b_event1614_minibuzz97.npy', 'STFT_F_gm267a_event1345_buzz347.npy', 'STFT_F_gm208a_event239_minibuzz6.npy', 'STFT_F_gm267a_event1285_buzz324.npy', 'STFT_F_gm187b_event1625_minibuzz103.npy', 'STFT_F_gm267a_event272_buzz9.npy', 'STFT_F_gm185b_event276_minibuzz88.npy', 'STFT_F_gm266a_event1047_buzz363.npy', 'STFT_F_gm187b_event2102_minibuzz129.npy', 'STFT_F_gm267a_event782_buzz229.npy', 'STFT_F_gm266a_event179_minibuzz3.npy', 'STFT_F_gm208a_event631_buzz75.npy', 'STFT_F_gm185b_event207_minibuzz48.npy', 'STFT_F_gm266a_event1000_buzz328.npy', 'STFT_F_gm187b_event2318_minibuzz150.npy', 'STFT_F_gm209a_event53_buzz16.npy', 'STFT_F_gm185b_event274_minibuzz86.npy', 'STFT_F_gm266a_event1179_buzz474.npy', 'STFT_F_gm187b_event2140_minibuzz132.npy', 'STFT_F_gm267a_event1252_buzz319.npy', 'STFT_F_gm185b_event258_minibuzz78.npy', 'STFT_F_gm266a_event1167_buzz462.npy', 'STFT_F_gm185b_event230_minibuzz62.npy', 'STFT_F_gm209c_event99_buzz7.npy', 'STFT_F_gm185b_event196_minibuzz41.npy', 'STFT_F_gm209a_event94_buzz28.npy', 'STFT_F_gm187b_event2421_minibuzz161.npy', 'STFT_F_gm266a_event818_buzz205.npy', 'STFT_F_gm185b_event380_minibuzz133.npy', 'STFT_F_gm187b_event1867_buzz212.npy', 'STFT_F_gm185b_event419_minibuzz150.npy', 'STFT_F_gm209c_event195_buzz10.npy', 'STFT_F_gm208a_event1153_minibuzz30.npy', 'STFT_F_gm267a_event1041_buzz277.npy', 'STFT_F_gm186b_event38_minibuzz2.npy', 'STFT_F_gm267a_event1625_buzz426.npy', 'STFT_F_gm185b_event198_minibuzz42.npy', 'STFT_F_gm209a_event26_buzz3.npy', 'STFT_F_gm185b_event173_minibuzz25.npy', 'STFT_F_gm266a_event1127_buzz430.npy', 'STFT_F_gm185b_event168_minibuzz20.npy', 'STFT_F_gm266a_event1297_buzz554.npy', 'STFT_F_gm187b_event526_minibuzz41.npy', 'STFT_F_gm187b_event169_buzz31.npy', 'STFT_F_gm208a_event445_minibuzz16.npy', 'STFT_F_gm267a_event1622_buzz423.npy', 'STFT_F_gm187b_event372_minibuzz38.npy', 'STFT_F_gm267a_event735_buzz204.npy', 'STFT_F_gm187b_event552_minibuzz46.npy', 'STFT_F_gm187b_event657_buzz81.npy', 'STFT_F_gm187b_event1603_minibuzz92.npy', 'STFT_F_gm208a_event466_buzz39.npy', 'STFT_F_gm185b_event187_minibuzz35.npy', 'STFT_F_gm266a_event388_buzz57.npy', 'STFT_F_gm187b_event1601_minibuzz90.npy', 'STFT_F_gm187b_event1152_buzz133.npy', 'STFT_F_gm187b_event558_minibuzz49.npy', 'STFT_F_gm266a_event988_buzz316.npy', 'STFT_F_gm187b_event2435_minibuzz167.npy', 'STFT_F_gm267a_event456_buzz64.npy', 'STFT_F_gm185b_event146_minibuzz1.npy', 'STFT_F_gm266a_event531_buzz97.npy', 'STFT_F_gm185b_event401_minibuzz143.npy', 'STFT_F_gm266a_event822_buzz209.npy', 'STFT_F_gm185b_event372_minibuzz125.npy', 'STFT_F_gm187b_event1951_buzz216.npy', 'STFT_F_gm187b_event2540_minibuzz173.npy', 'STFT_F_gm266a_event1031_buzz354.npy', 'STFT_F_gm187b_event1696_minibuzz109.npy', 'STFT_F_gm267a_event1565_buzz415.npy', 'STFT_F_gm187b_event2440_minibuzz168.npy', 'STFT_F_gm187b_event1321_buzz159.npy', 'STFT_F_gm187b_event2442_minibuzz169.npy', 'STFT_F_gm267a_event351_buzz34.npy', 'STFT_F_gm187b_event83_minibuzz17.npy', 'STFT_F_gm267a_event598_buzz105.npy', 'STFT_F_gm208a_event791_minibuzz25.npy', 'STFT_F_gm267a_event666_buzz153.npy', 'STFT_F_gm185b_event385_minibuzz136.npy', 'STFT_F_gm187b_event1710_buzz207.npy', 'STFT_F_gm185b_event601_minibuzz170.npy', 'STFT_F_gm266a_event1061_buzz377.npy', 'STFT_F_gm185b_event310_minibuzz100.npy', 'STFT_F_gm267a_event345_buzz28.npy', 'STFT_F_gm185b_event298_minibuzz93.npy', 'STFT_F_gm266a_event1052_buzz368.npy', 'STFT_F_gm187b_event82_minibuzz16.npy', 'STFT_F_gm267a_event657_buzz144.npy', 'STFT_F_gm208a_event458_minibuzz18.npy', 'STFT_F_gm187b_event1523_buzz195.npy', 'STFT_F_gm187b_event2396_minibuzz154.npy', 'STFT_F_gm266a_event1036_buzz359.npy', 'STFT_F_gm187b_event2548_minibuzz174.npy', 'STFT_F_gm266a_event807_buzz194.npy', 'STFT_F_gm187b_event2401_minibuzz158.npy', 'STFT_F_gm266a_event926_buzz267.npy', 'STFT_F_gm185b_event214_minibuzz52.npy', 'STFT_F_gm267a_event779_buzz226.npy', 'STFT_F_gm187b_event2316_minibuzz149.npy', 'STFT_F_gm267a_event601_buzz108.npy', 'STFT_F_gm187b_event2190_minibuzz139.npy', 'STFT_F_gm208a_event436_buzz30.npy', 'STFT_F_gm187b_event892_minibuzz70.npy', 'STFT_F_gm267a_event565_buzz100.npy', 'STFT_F_gm185b_event301_minibuzz96.npy', 'STFT_F_gm266a_event1322_buzz562.npy', 'STFT_F_gm185b_event421_minibuzz152.npy', 'STFT_F_gm267a_event1540_buzz390.npy', 'STFT_F_gm187b_event1621_minibuzz101.npy', 'STFT_F_gm266a_event992_buzz320.npy', 'STFT_F_gm208a_event1161_minibuzz33.npy', 'STFT_F_gm266a_event1227_buzz508.npy', 'STFT_F_gm187b_event2398_minibuzz156.npy', 'STFT_F_gm208a_event645_buzz79.npy', 'STFT_F_gm187b_event2216_minibuzz144.npy', 'STFT_F_gm267a_event1421_buzz354.npy', 'STFT_F_gm185b_event542_minibuzz168.npy', 'STFT_F_gm209a_event95_buzz29.npy', 'STFT_F_gm187b_event531_minibuzz42.npy', 'STFT_F_gm266a_event1236_buzz517.npy', 'STFT_F_gm185b_event156_minibuzz9.npy', 'STFT_F_gm267a_event1025_buzz275.npy', 'STFT_F_gm208a_event391_minibuzz9.npy', 'STFT_F_gm267a_event602_buzz109.npy', 'STFT_F_gm185b_event318_minibuzz104.npy', 'STFT_F_gm266a_event955_buzz289.npy', 'STFT_F_gm187b_event2338_minibuzz152.npy', 'STFT_F_gm266a_event1058_buzz374.npy']\n",
      "changed to aws_pilotwhales2 folder\n",
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025]\n",
      "[559, 94, 784, 75, 1047, 90, 484, 158, 503, 90, 559, 109, 334, 68, 1722, 75, 559, 75, 409, 83, 934, 83, 2059, 128, 447, 60, 597, 79, 1272, 102, 203, 240, 559, 98, 1197, 109, 897, 68, 702, 38, 859, 90, 1572, 57, 1084, 75, 672, 94, 484, 60, 634, 83, 409, 105, 484, 64, 784, 68, 447, 94, 1384, 57, 784, 83, 822, 79, 522, 53, 522, 57, 297, 68, 859, 75, 447, 120, 604, 75, 1047, 49, 447, 94, 672, 64, 334, 45, 1114, 57, 672, 90, 297, 57, 259, 57, 1197, 68, 934, 75, 597, 60, 559, 75, 1047, 49, 1272, 57, 522, 72, 597, 90, 829, 75, 1684, 75, 672, 72, 1009, 53, 934, 68, 1009, 30, 859, 53, 559, 83, 484, 90, 634, 57, 559, 83, 859, 49, 484, 98, 634, 53, 597, 83, 859, 72, 334, 49, 784, 147, 709, 53, 848, 68, 297, 53, 972, 109, 184, 53, 529, 72, 934, 53, 747, 49, 522, 57, 559, 64, 597, 64, 522]\n",
      "2059\n",
      "first loop done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second loop done\n",
      "hotlabels and samples in np arrays\n"
     ]
    }
   ],
   "source": [
    "#import training data\n",
    "#see getLabels_stackedData3.py for preprocessing\n",
    "\n",
    "#change to the directory where list of STFT files is \n",
    "print(os.getcwd())\n",
    "os.chdir('/home/ec2-user/SageMaker')\n",
    "print(os.getcwd())\n",
    "\n",
    "labels = []\n",
    "allCols = []\n",
    "allRows = []\n",
    "allSTFTs = []\n",
    "paddedSTFTs = []\n",
    "maxTime = 0\n",
    "print('made variables')\n",
    "\n",
    "#open file (lists all train .npy STFT file names) \n",
    "file = open('TopList.txt', 'r')\n",
    "data = file.read().split('\\n')\n",
    "file.close()\n",
    "print('got data files in a list')\n",
    "print(data)\n",
    "\n",
    "os.chdir('./aws_pilotwhales2')\n",
    "print('changed to aws_pilotwhales2 folder')\n",
    "\n",
    "for array in data:\n",
    "    #loop through to find out if file is a buzz or minibuzz, and add label accordingly \n",
    "    nameParse = array.split(\"_\",-1)\n",
    "    #print(nameParse)\n",
    "    typeParse = nameParse[4].split(\"u\", -1)\n",
    "    #print(typeParse)\n",
    "    if(typeParse[0] == 'b'):\n",
    "        #buzz = 1 \n",
    "        labels.append(1)\n",
    "    elif(typeParse[0] == 'minib'):\n",
    "        #minibuzz = 0\n",
    "        labels.append(0)\n",
    "    else: \n",
    "        print('Error, not a buzz or minibuzz!')\n",
    "    \n",
    "    curSTFT = np.load(array)\n",
    "        \n",
    "    #find cols (number of time steps) of each STFT and save longest one\n",
    "    rows, cols = curSTFT.shape\n",
    "    allCols.append(cols)\n",
    "    allRows.append(rows)\n",
    "    if (cols>maxTime):\n",
    "        maxTime = cols\n",
    "    if (rows!=1025):\n",
    "        print('Error, not 1025 STFT coefficients') \n",
    "        \n",
    "    allSTFTs.append(curSTFT)\n",
    "    \n",
    "print(labels)\n",
    "print(allRows)\n",
    "print(allCols)\n",
    "print(maxTime) \n",
    "print('first loop done')\n",
    "\n",
    "for array in data: \n",
    "    #loop though STFTs again to zero pad, transpose, and reshape soo that there is one channel  \n",
    "    #NOTE: must be done after we definitvely know the max number of time steps\n",
    "    curSTFT = np.load(array)\n",
    "    rows, cols = curSTFT.shape\n",
    "    pad = maxTime-cols\n",
    "    \n",
    "    zeroPad = np.zeros((rows,maxTime-cols))\n",
    "    paddedSTFT = np.append(curSTFT, zeroPad, axis = 1)\n",
    "    \n",
    "    paddedSTFT = np.transpose(paddedSTFT)\n",
    "    paddedSTFT = np.reshape(paddedSTFT, paddedSTFT.shape + (1,))\n",
    "    \n",
    "    paddedSTFTs.append(paddedSTFT)\n",
    "\n",
    "print('second loop done')\n",
    "\n",
    "labels = np.array(labels)\n",
    "#one hot encoding for labels\n",
    "hotlabels = keras.utils.to_categorical(labels, num_classes=2, dtype='float32')\n",
    "samples = np.array(paddedSTFTs)\n",
    "print('hotlabels and samples in np arrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to start model\n",
      "made model\n",
      "layer 1 done\n",
      "layer 2 done\n",
      "layer 3 done\n",
      "layer 4 done\n",
      "layer 5 done\n",
      "layer 6 done\n",
      "compiled\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 2059, 1, 256)      262400    \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 2059, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1030, 1, 256)      196608    \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 1030, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 515, 1, 256)       196608    \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 515, 1, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 258, 1, 256)       196608    \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 258, 1, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 512       \n",
      "=================================================================\n",
      "Total params: 852,736\n",
      "Trainable params: 852,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#layer 0: input\n",
    "#labels[]\n",
    "#paddedSTFTs[] (maxTime rows, 1025 cols) each\n",
    "\n",
    "print('ready to start model')\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "\n",
    "print('made model')\n",
    "\n",
    "#NOTE, CHANGED PADDING ON 2D CONVOLUTIONS from 'valid'=no paddinng to 'same'=padding so input and output are same dimensions\n",
    "\n",
    "#layer 1: 2D convolution between input and 256 filters with 1 row and 1025 cols\n",
    "model.add(Conv2D(256, input_shape = [maxTime,1025,1], kernel_size = [1,1025], strides=(1, 1), padding='valid', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "#batch normalization- add in layer? don't understand parameters well\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "#reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 1 done')\n",
    "\n",
    "##layer 2: 2D convolution between output of layer 1 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 2 done')\n",
    "\n",
    "#layer 3: 2D convolution between output of layer 2 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 3 done')\n",
    "\n",
    "#layer 4: 2D convolution between output of layer 3 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 4 done')\n",
    "\n",
    "#layer 5: Global max pooling\n",
    "model.add(GlobalMaxPooling2D(data_format=\"channels_last\"))\n",
    "print('layer 5 done')\n",
    "\n",
    "#layer 6: fully connected layer\n",
    "model.add(Dense(2, activation='softmax', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "print('layer 6 done')\n",
    "\n",
    "#Compile model [COMPILE]\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "print('compiled')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135 samples, validate on 34 samples\n",
      "Epoch 1/3\n",
      " - 14s - loss: 0.5891 - acc: 0.6444 - val_loss: 0.2355 - val_acc: 0.8824\n",
      "Epoch 2/3\n",
      " - 13s - loss: 0.2551 - acc: 0.8963 - val_loss: 0.3511 - val_acc: 0.8529\n",
      "Epoch 3/3\n",
      " - 13s - loss: 0.1971 - acc: 0.9259 - val_loss: 0.1071 - val_acc: 0.9706\n",
      "ran fit\n"
     ]
    }
   ],
   "source": [
    "#Now let us train our model [FIT]\n",
    "ES = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "\n",
    "model.fit(x=samples, y=hotlabels, batch_size=26, epochs=3, verbose=2, callbacks=[ES], validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "print('ran fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testSamples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fd4bf19f8e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#preprocess STFT data in TestData folder!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#see getLabels_stackedData3.py, and give labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestHotlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testSamples' is not defined"
     ]
    }
   ],
   "source": [
    "#[EVALUATE]\n",
    "#do for test data, not training data! \n",
    "#preprocess STFT data in TestData folder!\n",
    "#see getLabels_stackedData3.py, and give labels\n",
    "#results = model.evaluate(x=testSamples, y=testHotlabels, batch_size=26, verbose=1, sample_weight=None, steps=None)\n",
    "results = model.evaluate(x=samples, y=hotlabels, batch_size=26, verbose=1, sample_weight=None, steps=None)\n",
    "print(results)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 5s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "#[PREDICT] (w/TestData I kept aside as well)\n",
    "#preprocess STFT data in TestData folder!\n",
    "#see getLabels_stackedData3.py, but don't give labels\n",
    "hotlabels_pred = model.predict(samples, batch_size=26, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False]\n"
     ]
    }
   ],
   "source": [
    "#see if prediction results are right (compare hot labels I generate with what the model guesses)\n",
    "print((np.max(abs(hotlabels-hotlabels_pred),axis=1)>.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
