{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import things to use \n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Masking, Dense \n",
    "from keras.layers import Convolution2D as Conv2D \n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D \n",
    "from keras.layers import Softmax, ReLU, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STFT_F_gm266a_event1016_buzz344.npy', 'STFT_F_gm185b_event415_minibuzz148.npy', 'STFT_F_gm267a_event706_buzz176.npy', 'STFT_F_gm208a_event236_minibuzz4.npy', 'STFT_F_gm266a_event939_buzz280.npy', 'STFT_F_gm185b_event423_minibuzz154.npy']\n",
      "[1, 0, 1, 0, 1, 0]\n",
      "[1025, 1025, 1025, 1025, 1025, 1025]\n",
      "[597, 79, 634, 30, 1084, 64]\n",
      "1084\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import training data\n",
    "#see getLabels_stackedData3.py for preprocessing\n",
    "\n",
    "#run for train data only\n",
    "path = './'\n",
    "os.chdir(path)\n",
    "\n",
    "labels = []\n",
    "allCols = []\n",
    "allRows = []\n",
    "allSTFTs = []\n",
    "paddedSTFTs = []\n",
    "maxTime = 0\n",
    "\n",
    "\n",
    "#open file (lists all test/train .npy STFT file names) \n",
    "file = open('list.txt', 'r')\n",
    "data = file.read().split('\\n')\n",
    "file.close()\n",
    "print(data)\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "for array in data:\n",
    "    #loop through to find out if file is a buzz or minibuzz, and add label accordingly \n",
    "    nameParse = array.split(\"_\",-1)\n",
    "    typeParse = nameParse[4].split(\"u\", -1)\n",
    "    if(typeParse[0] == 'b'):\n",
    "        labels.append(1)\n",
    "    elif(typeParse[0] == 'minib'):\n",
    "        labels.append(0)\n",
    "    else: \n",
    "        print('Error, not a buzz or minibuzz!')\n",
    "    \n",
    "    curSTFT = np.load(array)\n",
    "        \n",
    "    #find cols (number of time steps) of each STFT and save longest one\n",
    "    rows, cols = curSTFT.shape\n",
    "    allCols.append(cols)\n",
    "    allRows.append(rows)\n",
    "    if (cols>maxTime):\n",
    "        maxTime = cols\n",
    "    if (rows!=1025):\n",
    "        print('Error, not 1025 STFT coefficients') \n",
    "        \n",
    "    allSTFTs.append(curSTFT)\n",
    "    \n",
    "print(labels)\n",
    "print(allRows)\n",
    "print(allCols)\n",
    "print(maxTime) \n",
    "\n",
    "for array in data: \n",
    "    #loop though STFTs again to zero pad and transpose  \n",
    "    #NOTE: must be done after we definitvely know the max number of time steps\n",
    "    curSTFT = np.load(array)\n",
    "    rows, cols = curSTFT.shape\n",
    "    pad = maxTime-cols\n",
    "    \n",
    "    zeroPad = np.zeros((rows,maxTime-cols))\n",
    "    paddedSTFT = np.append(curSTFT, zeroPad, axis = 1)\n",
    "    \n",
    "    paddedSTFT = np.transpose(paddedSTFT)\n",
    "    paddedSTFT = np.reshape(paddedSTFT, paddedSTFT.shape + (1,))\n",
    "    \n",
    "    paddedSTFTs.append(paddedSTFT)\n",
    "    \n",
    "\n",
    "labels = np.array(labels)\n",
    "hotlabels = keras.utils.to_categorical(labels, num_classes=2, dtype='float32')\n",
    "samples = np.array(paddedSTFTs)\n",
    "print(hotlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to start model\n",
      "made model\n",
      "layer 1 done\n",
      "layer 2 done\n",
      "layer 3 done\n",
      "layer 4 done\n",
      "layer 5 done\n",
      "layer 6 done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SGD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a5be05e26570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#OLD COMPILE (for fit, not fit_generator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#note, can play with leraning rate and other parameters here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'compiled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
     ]
    }
   ],
   "source": [
    "#MAKE MODEL AND COMPILE IT\n",
    "\n",
    "#layer 0: input\n",
    "#labels[], samples[] (maxTime rows, 1025 cols) each in trainGen and validGen\n",
    "print('ready to start model')\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "print('made model')\n",
    "\n",
    "#NOTE, CHANGED PADDING ON 2D CONVOLUTIONS from 'valid'=no paddinng to 'same'=padding so input and output are same dimensions\n",
    "\n",
    "#layer 1: 2D convolution between input and 256 filters with 1 row and 1025 cols\n",
    "model.add(Conv2D(256, input_shape = [maxTime,1025,1], kernel_size = [1,1025], strides=(1, 1), padding='valid', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "#batch normalization- add in layer? don't understand parameters well\n",
    "#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "#reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 1 done')\n",
    "\n",
    "##layer 2: 2D convolution between output of layer 1 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 2 done')\n",
    "\n",
    "#layer 3: 2D convolution between output of layer 2 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 3 done')\n",
    "\n",
    "#layer 4: 2D convolution between output of layer 3 and 256 filters with 3 rows and 256 cols\n",
    "model.add(Conv2D(256, kernel_size = [3,1], strides=(2, 1), padding='same', data_format=\"channels_last\", dilation_rate=(1, 1), activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "##batch normalization- add in layer? don't understand parameters well\n",
    "##model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "##reLU layer\n",
    "model.add(ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
    "print('layer 4 done')\n",
    "\n",
    "#layer 5: Global max pooling\n",
    "model.add(GlobalMaxPooling2D(data_format=\"channels_last\"))\n",
    "print('layer 5 done')\n",
    "\n",
    "#layer 6: fully connected layer\n",
    "model.add(Dense(2, activation='softmax', use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "print('layer 6 done')\n",
    "\n",
    "#Compile model [COMPILE]\n",
    "#OLD COMPILE (for fit, not fit_generator)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "#opt = SGD(lr=0.02) #note, can play with leraning rate and other parameters here\n",
    "#model.compile(loss = \"binary_crossentropy\", optimizer = opt, metrics=[\"accuracy\"])\n",
    "print('compiled')\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let us train our model [FIT]\n",
    "ES = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=2, mode='auto', baseline=None, restore_best_weights=True)\n",
    "#test\n",
    "model.fit(x=samples, y=hotlabels, batch_size=1, epochs=6, verbose=2, callbacks=[ES], validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#actual\n",
    "#model.fit(x=samples, y=labels, batch_size=26, epochs=26, verbose=2, callbacks=ES, validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "print('ran fit')\n",
    "\n",
    "#[EVALUATE]\n",
    "#test\n",
    "model.evaluate(x=samples, y=hotlabels, batch_size=1, verbose=1, sample_weight=None, steps=None)\n",
    "#actual\n",
    "#model.evaluate(x=samples, y=labels, batch_size=26, verbose=1, sample_weight=None, steps=None, callbacks=ES)\n",
    "\n",
    "#[PREDICT (w/TestData I kept aside)]\n",
    "#preprocess STFT data in TestData folder!\n",
    "#see getLabels_stackedData3.py, but don't give labels\n",
    "##model.predict(testSamples, batch_size=26, verbose=1, steps=None, callbacks=ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
